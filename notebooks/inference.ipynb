{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f204b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms, models\n",
    "import torch.nn.functional as F\n",
    "from torchvision.ops import nms\n",
    "import cv2\n",
    "import subprocess\n",
    "import tempfile\n",
    "import time\n",
    "import json\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40419864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "project_root= os.getcwd() + \"/../\"\n",
    "src_path= os.path.join(project_root, 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "from config import images_val_dir, labels_2_dir, artifacts_dir\n",
    "import config\n",
    "from preprocessing import FaceDataset, filter_valid_bboxes, apply_deltas_to_boxes, clamp_boxes_to_img_boundary\n",
    "\n",
    "from utils import visualize_anchors_and_gt\n",
    "from models import RPN, RoIHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "992852c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atlas/playground/FasterRCNNFaceDetection/face_detection_env/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/atlas/playground/FasterRCNNFaceDetection/face_detection_env/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "vgg16= models.vgg16(pretrained= True)\n",
    "backbone= vgg16.features[:-1]\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad= False\n",
    "rpn_model= RPN()\n",
    "rpn_model.load_state_dict(torch.load(artifacts_dir + \"rpn_5epchs_vgg16bb_s2_r1_lr1e-4_wghts_bs16.pth\"))\n",
    "for p in rpn_model.parameters():\n",
    "    p.requires_grad= False\n",
    "roi_head= RoIHead(in_channels= 512, num_classes= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c48a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path= os.path.join(artifacts_dir, \"vgg16_backbone.pth\")\n",
    "torch.save(backbone, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfe76f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_head.load_state_dict(torch.load(artifacts_dir + \"roi_10eps_rpns2r1_2048fc_512p_07fg_015val_wghts.pth\"))\n",
    "for p in roi_head.parameters():\n",
    "    p.requires_grad= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fedfebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def roi_head_inference(features, proposals, image_shape, roi_head, score_thresh= 0.5, nms_thresh= 0.1):\n",
    "    roi_out= roi_head(features, [proposals], [image_shape], gt_boxes= None)\n",
    "    cls_logits= roi_out['cls_logits']\n",
    "    bbox_deltas= roi_out['bbox_deltas']\n",
    "\n",
    "    print(\"cls_logits range:\", cls_logits.min().item(), cls_logits.max().item())\n",
    "    scores= F.softmax(cls_logits, dim= 1)[:, 1]\n",
    "    print(scores.max())\n",
    "    pred_boxes= apply_deltas_to_boxes(proposals, bbox_deltas)\n",
    "    pred_boxes= clamp_boxes_to_img_boundary(pred_boxes, image_shape)\n",
    "\n",
    "    keep= scores > score_thresh\n",
    "    pred_boxes= pred_boxes[keep]\n",
    "    scores= scores[keep]\n",
    "\n",
    "    keep= nms(pred_boxes, scores, nms_thresh)\n",
    "    return pred_boxes[keep], scores[keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8cccd2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_proposals(images, image_shapes):\n",
    "    features= backbone(images)\n",
    "    with torch.cuda.amp.autocast():\n",
    "      rpn_out= rpn_model(feat= features, image_shapes= image_shapes, gt_boxes= None)\n",
    "    proposals= [p[:512] for p in rpn_out['proposals']]\n",
    "    return proposals, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a82904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame, backbone, rpn_model,\n",
    "                  roi_head, device, score_thresh=0.5, nms_thresh=0.1):\n",
    "    orig_h, orig_w = frame.shape[:2]\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # transform = transforms.Compose([\n",
    "    #     transforms.ToPILImage(),\n",
    "    #     transforms.Resize((512, 512)),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    # ])\n",
    "    frame_resized= cv2.resize(frame_rgb, (512, 512))\n",
    "    frame_tensor= torch.from_numpy(frame_resized).float() / 255.0\n",
    "    frame_tensor= frame_tensor.permute(2, 0, 1)\n",
    "    frame_tensor= transforms.Normalize(mean= [0.485, 0.456, 0.406], std= [0.229, 0.224, 0.225])(frame_tensor)\n",
    "    input_tensor = frame_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "    _, _, H, W = input_tensor.shape\n",
    "    image_shapes = [(H, W)]\n",
    "\n",
    "    proposals, features = generate_proposals(input_tensor, image_shapes, backbone, rpn_model)\n",
    "    proposals = proposals[0]\n",
    "\n",
    "    pred_boxes, pred_scores = roi_head_inference(\n",
    "        features, proposals, (H, W), roi_head, score_thresh=score_thresh, nms_thresh=nms_thresh\n",
    "    )\n",
    "\n",
    "    scale_x = orig_w / W\n",
    "    scale_y = orig_h / H\n",
    "\n",
    "    pred_boxes[:, 0] *= scale_x\n",
    "    pred_boxes[:, 1] *= scale_y\n",
    "    pred_boxes[:, 2] *= scale_x\n",
    "    pred_boxes[:, 3] *= scale_y\n",
    "\n",
    "    return pred_boxes, pred_scores\n",
    "\n",
    "def get_video_info(video_path):\n",
    "    cmd = [\n",
    "        \"ffprobe\",\n",
    "        \"-v\", \"quiet\",\n",
    "        \"-print_format\", \"json\",\n",
    "        \"-show_format\",\n",
    "        \"-show_streams\",\n",
    "        video_path\n",
    "    ]\n",
    "    try:\n",
    "        # Corrected: removed extra () after subprocess.run\n",
    "        result = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        info = json.loads(result.stdout)\n",
    "        video_stream = next((stream for stream in info['streams'] if stream['codec_type'] == 'video'), None)\n",
    "        if video_stream:\n",
    "            # Safely get fps, handling cases where it might be a string like \"30/1\"\n",
    "            try:\n",
    "                fps = float(eval(video_stream.get('r_frame_rate', '30/1')))\n",
    "            except (SyntaxError, NameError):\n",
    "                fps = 30.0 # Default if evaluation fails\n",
    "\n",
    "            width = video_stream['width']\n",
    "            height = video_stream['height']\n",
    "            return fps, width, height\n",
    "        return None, None, None\n",
    "    except (subprocess.CalledProcessError, json.JSONDecodeError, KeyError, StopIteration) as e:\n",
    "        # Corrected typo: gettinv -> getting\n",
    "        print(f\"Error getting video info: {e}\") \n",
    "        return None, None, None\n",
    "\n",
    "def process_video(\n",
    "        video_path, backbone, rpn_model, \n",
    "        roi_head, device, output_path=None, # output_path is not used for real-time display, consider removing or making explicit\n",
    "        score_thresh=0.5, nms_thresh=0.5, sample_every=1):\n",
    "    \n",
    "    # Create temporary directory for frames (still useful for general cleanup, even if not saving frames)\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    \n",
    "    ffmpeg_process = None # Initialize to None for the finally block\n",
    "    \n",
    "    try:\n",
    "        fps, width, height = get_video_info(video_path)\n",
    "        if fps is None:\n",
    "            print(\"Couldn't get video info, using defaults\")\n",
    "            fps = 30.0 # Ensure float for calculations\n",
    "            # If width/height are also None, default them for robust operation\n",
    "            # You might need to set a sensible default or raise an error\n",
    "            # For demonstration, let's assume get_video_info returns valid width/height\n",
    "            # If not, you might need to infer or hardcode a typical video size.\n",
    "            # For this example, assuming they are set by get_video_info or handle this case.\n",
    "            if width is None or height is None:\n",
    "                raise ValueError(\"Could not get video dimensions from ffprobe. Please ensure ffprobe is installed and video is valid.\")\n",
    "\n",
    "\n",
    "        print(f\"Video info: FPS={fps}, Width={width}, Height={height}\")\n",
    "\n",
    "        ffmpeg_cmd = [\n",
    "            \"ffmpeg\",\n",
    "            \"-i\", video_path,\n",
    "            \"-f\", \"image2pipe\",\n",
    "            \"-pix_fmt\", \"bgr24\",\n",
    "            \"-vcodec\", \"rawvideo\",\n",
    "            \"-\"\n",
    "        ]\n",
    "\n",
    "        # Use bufsize=10**8 for larger buffer, helps with performance\n",
    "        ffmpeg_process = subprocess.Popen(ffmpeg_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=10**8)\n",
    "    \n",
    "        backbone.eval()\n",
    "        rpn_model.eval()\n",
    "        roi_head.eval()\n",
    "\n",
    "        frame_count = 0        # Total frames read from pipe\n",
    "        processed_count = 0    # Total frames processed and displayed\n",
    "        \n",
    "        frame_size = width * height * 3 # Bytes per frame (BGR)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        last_display_time = time.time() # For instantaneous FPS\n",
    "        print(\"Processing video ... press 'q' to quit\")\n",
    "\n",
    "        while True:\n",
    "            # Read raw frame data from ffmpeg\n",
    "            raw_frame_data = ffmpeg_process.stdout.read(frame_size)\n",
    "            \n",
    "            if len(raw_frame_data) != frame_size:\n",
    "                # This could be end of stream or an error\n",
    "                if ffmpeg_process.poll() is not None and len(raw_frame_data) == 0:\n",
    "                    print(\"FFmpeg process finished, end of video stream.\")\n",
    "                elif len(raw_frame_data) > 0:\n",
    "                    print(f\"Warning: Partial frame read. Expected {frame_size} bytes, got {len(raw_frame_data)}.\")\n",
    "                else:\n",
    "                    print(\"No more data from ffmpeg stdout, breaking loop.\")\n",
    "                break\n",
    "\n",
    "            frame = np.frombuffer(raw_frame_data, dtype=np.uint8).reshape((height, width, 3)).copy()\n",
    "\n",
    "            if frame_count % sample_every == 0:\n",
    "                boxes, scores = process_frame(\n",
    "                    frame, backbone, rpn_model,\n",
    "                    roi_head, device, score_thresh,\n",
    "                    nms_thresh\n",
    "                )\n",
    "                \n",
    "                for box, score in zip(boxes, scores):\n",
    "                    x1, y1, x2, y2 = box.int().tolist()\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "                    # Corrected: y-coordinate offset for text\n",
    "                    cv2.putText(frame, f\"{score:.2f}\", (x1, y1 - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                \n",
    "                current_time = time.time()\n",
    "                \n",
    "                # Calculate and display instantaneous FPS\n",
    "                elapsed_display = current_time - last_display_time\n",
    "                if elapsed_display > 0:\n",
    "                    fps_display = 1.0 / elapsed_display\n",
    "                    cv2.putText(frame, f\"FPS: {fps_display:.1f}\", (10, 30), # Top-left corner\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                last_display_time = current_time\n",
    "\n",
    "                cv2.imshow('Object Detection', frame)\n",
    "\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    print(\"'q' pressed, quitting.\")\n",
    "                    break\n",
    "                \n",
    "                processed_count += 1\n",
    "\n",
    "                # Print average FPS periodically for console output\n",
    "                if processed_count % 10 == 0:\n",
    "                    elapsed_total = current_time - start_time\n",
    "                    avg_fps_total = processed_count / elapsed_total if elapsed_total > 0 else 0\n",
    "                    print(f\"Processed {processed_count} frames, Average FPS: {avg_fps_total:.1f}\")\n",
    "            \n",
    "            frame_count += 1 # Increment total frame count regardless of sampling\n",
    "\n",
    "        # Final statistics after loop ends\n",
    "        total_time = time.time() - start_time\n",
    "        avg_fps = processed_count / total_time if total_time > 0 else 0\n",
    "        print(f\"Processing complete. Processed {processed_count} frames (out of {frame_count} total read) in {total_time:.1f} seconds\")\n",
    "        print(f\"Overall Average FPS: {avg_fps:.1f}\")\n",
    "    \n",
    "    finally:\n",
    "        if ffmpeg_process and ffmpeg_process.poll() is None:\n",
    "            ffmpeg_process.terminate()\n",
    "            ffmpeg_process.wait(timeout=5) # Give it a moment to terminate\n",
    "            if ffmpeg_process.poll() is None:\n",
    "                ffmpeg_process.kill() # Force kill if still not terminated\n",
    "        cv2.destroyAllWindows()\n",
    "        if os.path.exists(temp_dir):\n",
    "            shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e9882be",
   "metadata": {},
   "outputs": [],
   "source": [
    "vids_dir= project_root + \"test_vids/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b84ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video info: FPS=24.0, Width=1920, Height=1080\n",
      "Processing video ... press 'q' to quit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74519/3023498800.py:4: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_logits range: -1.428232192993164 1.3761736154556274\n",
      "tensor(0.0832)\n",
      "cls_logits range: -1.428232192993164 1.3761736154556274\n",
      "tensor(0.0831)\n",
      "cls_logits range: -1.428232192993164 1.3761736154556274\n",
      "tensor(0.0831)\n",
      "cls_logits range: -1.428232192993164 1.3761736154556274\n",
      "tensor(0.0831)\n",
      "cls_logits range: -1.428232192993164 1.3761736154556274\n",
      "tensor(0.0831)\n",
      "cls_logits range: -1.428232192993164 1.3761736154556274\n",
      "tensor(0.0831)\n",
      "cls_logits range: -1.428232192993164 1.3761736154556274\n",
      "tensor(0.0831)\n",
      "cls_logits range: -1.428232192993164 1.3761736154556274\n",
      "tensor(0.0831)\n",
      "cls_logits range: -1.428232192993164 1.3761736154556274\n",
      "tensor(0.0831)\n",
      "cls_logits range: -1.428232192993164 1.3761736154556274\n",
      "tensor(0.0831)\n",
      "Processed 10 frames, Average FPS: 0.6\n",
      "cls_logits range: -1.428232192993164 1.3761736154556274\n",
      "tensor(0.0831)\n",
      "cls_logits range: -1.4347882270812988 1.379327416419983\n",
      "tensor(0.0831)\n",
      "cls_logits range: -1.4347882270812988 1.379327416419983\n",
      "tensor(0.0831)\n",
      "cls_logits range: -1.4347882270812988 1.379327416419983\n",
      "tensor(0.0831)\n",
      "cls_logits range: -1.4347882270812988 1.379327416419983\n",
      "tensor(0.0831)\n",
      "cls_logits range: -1.4631869792938232 1.3890844583511353\n",
      "tensor(0.0831)\n",
      "cls_logits range: -1.4650731086730957 1.3966490030288696\n",
      "tensor(0.0830)\n",
      "cls_logits range: -1.5171513557434082 1.4585975408554077\n",
      "tensor(0.0830)\n",
      "cls_logits range: -1.4805119037628174 1.4067212343215942\n",
      "tensor(0.0829)\n",
      "cls_logits range: -1.4924235343933105 1.4239290952682495\n",
      "tensor(0.0847)\n",
      "Processed 20 frames, Average FPS: 0.9\n",
      "cls_logits range: -1.486985445022583 1.418270468711853\n",
      "tensor(0.0852)\n",
      "cls_logits range: -1.505983591079712 1.4175180196762085\n",
      "tensor(0.0828)\n",
      "cls_logits range: -1.520409107208252 1.431113362312317\n",
      "tensor(0.0851)\n",
      "cls_logits range: -1.5154340267181396 1.4419530630111694\n",
      "tensor(0.0845)\n",
      "cls_logits range: -1.5062329769134521 1.471448302268982\n",
      "tensor(0.0855)\n",
      "cls_logits range: -1.5149431228637695 1.4741092920303345\n",
      "tensor(0.0874)\n",
      "cls_logits range: -1.5083520412445068 1.4563841819763184\n",
      "tensor(0.0853)\n",
      "cls_logits range: -1.5224177837371826 1.4432306289672852\n",
      "tensor(0.0866)\n",
      "cls_logits range: -1.515390157699585 1.457600474357605\n",
      "tensor(0.0855)\n",
      "cls_logits range: -1.5165846347808838 1.4640542268753052\n",
      "tensor(0.0888)\n",
      "Processed 30 frames, Average FPS: 1.0\n",
      "cls_logits range: -1.5093648433685303 1.4531177282333374\n",
      "tensor(0.1036)\n",
      "cls_logits range: -1.5129115581512451 1.4551249742507935\n",
      "tensor(0.1042)\n",
      "cls_logits range: -1.5178585052490234 1.4541343450546265\n",
      "tensor(0.1002)\n",
      "cls_logits range: -1.4998817443847656 1.4590891599655151\n",
      "tensor(0.1001)\n",
      "cls_logits range: -1.5088794231414795 1.454020380973816\n",
      "tensor(0.0904)\n",
      "cls_logits range: -1.5043866634368896 1.44827139377594\n",
      "tensor(0.0859)\n",
      "cls_logits range: -1.4974703788757324 1.4864476919174194\n",
      "tensor(0.0953)\n",
      "cls_logits range: -1.5020194053649902 1.481163740158081\n",
      "tensor(0.1275)\n",
      "cls_logits range: -1.5312812328338623 1.4820917844772339\n",
      "tensor(0.1004)\n",
      "cls_logits range: -1.499192237854004 1.4815500974655151\n",
      "tensor(0.1316)\n",
      "Processed 40 frames, Average FPS: 1.1\n",
      "cls_logits range: -1.4812097549438477 1.5154213905334473\n",
      "tensor(0.1178)\n",
      "cls_logits range: -1.4852633476257324 1.4861546754837036\n",
      "tensor(0.1401)\n",
      "cls_logits range: -1.492485523223877 1.4742814302444458\n",
      "tensor(0.1249)\n",
      "cls_logits range: -1.4848909378051758 1.4792033433914185\n",
      "tensor(0.1473)\n",
      "cls_logits range: -1.5020978450775146 1.4605921506881714\n",
      "tensor(0.1373)\n",
      "cls_logits range: -1.5308561325073242 1.4673117399215698\n",
      "tensor(0.1172)\n",
      "cls_logits range: -1.5120210647583008 1.4379981756210327\n",
      "tensor(0.0986)\n",
      "cls_logits range: -1.5280373096466064 1.4744905233383179\n",
      "tensor(0.1065)\n",
      "cls_logits range: -1.525618076324463 1.475820541381836\n",
      "tensor(0.1131)\n",
      "cls_logits range: -1.5322730541229248 1.4667136669158936\n",
      "tensor(0.1013)\n",
      "Processed 50 frames, Average FPS: 1.1\n",
      "cls_logits range: -1.5357697010040283 1.4623068571090698\n",
      "tensor(0.1055)\n",
      "'q' pressed, quitting.\n",
      "Processing complete. Processed 50 frames (out of 150 total read) in 45.3 seconds\n",
      "Overall Average FPS: 1.1\n"
     ]
    },
    {
     "ename": "TimeoutExpired",
     "evalue": "Command '['ffmpeg', '-i', '/home/atlas/playground/FasterRCNNFaceDetection/notebooks/../test_vids/trailer.mp4', '-f', 'image2pipe', '-pix_fmt', 'bgr24', '-vcodec', 'rawvideo', '-']' timed out after 5 seconds",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutExpired\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m rpn_model= rpn_model.to(device)\n\u001b[32m      6\u001b[39m roi_head= roi_head.to(device)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mvids_dir\u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrailer.mp4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrpn_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrpn_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroi_head\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mroi_head\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscore_thresh\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnms_thresh\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_every\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 185\u001b[39m, in \u001b[36mprocess_video\u001b[39m\u001b[34m(video_path, backbone, rpn_model, roi_head, device, output_path, score_thresh, nms_thresh, sample_every)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ffmpeg_process \u001b[38;5;129;01mand\u001b[39;00m ffmpeg_process.poll() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    184\u001b[39m     ffmpeg_process.terminate()\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[43mffmpeg_process\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Give it a moment to terminate\u001b[39;00m\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ffmpeg_process.poll() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    187\u001b[39m         ffmpeg_process.kill() \u001b[38;5;66;03m# Force kill if still not terminated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/subprocess.py:1264\u001b[39m, in \u001b[36mPopen.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1262\u001b[39m     endtime = _time() + timeout\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1265\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1266\u001b[39m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[32m   1267\u001b[39m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[32m   1268\u001b[39m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[32m   1269\u001b[39m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[32m   1270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/subprocess.py:2045\u001b[39m, in \u001b[36mPopen._wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   2043\u001b[39m remaining = \u001b[38;5;28mself\u001b[39m._remaining_time(endtime)\n\u001b[32m   2044\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2045\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m TimeoutExpired(\u001b[38;5;28mself\u001b[39m.args, timeout)\n\u001b[32m   2046\u001b[39m delay = \u001b[38;5;28mmin\u001b[39m(delay * \u001b[32m2\u001b[39m, remaining, \u001b[32m.05\u001b[39m)\n\u001b[32m   2047\u001b[39m time.sleep(delay)\n",
      "\u001b[31mTimeoutExpired\u001b[39m: Command '['ffmpeg', '-i', '/home/atlas/playground/FasterRCNNFaceDetection/notebooks/../test_vids/trailer.mp4', '-f', 'image2pipe', '-pix_fmt', 'bgr24', '-vcodec', 'rawvideo', '-']' timed out after 5 seconds"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "vids_dir= project_root + \"test_vids/\"\n",
    "# device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device= torch.device('cpu')\n",
    "backbone= backbone.to(device)\n",
    "rpn_model= rpn_model.to(device)\n",
    "roi_head= roi_head.to(device)\n",
    "process_video(\n",
    "    video_path= vids_dir+ \"trailer.mp4\",\n",
    "    backbone= backbone,\n",
    "    rpn_model= rpn_model,\n",
    "    roi_head= roi_head,\n",
    "    device= device,\n",
    "    score_thresh= 0.5,\n",
    "    nms_thresh= 0.1,\n",
    "    sample_every= 3\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_detection_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
