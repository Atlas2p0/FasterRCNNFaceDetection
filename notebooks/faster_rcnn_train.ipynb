{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ec03d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "from torch.optim import Adam\n",
    "from torchvision.ops import nms, RoIAlign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c861ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "notebook_dir= os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "project_root= os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "\n",
    "src_path= os.path.join(project_root, 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "from config import data_dir, images_train_dir, images_val_dir, labels_train_dir, labels_val_dir, artifacts_dir\n",
    "import config\n",
    "from preprocessing import FaceDataset, generate_anchor_boxes, calculate_iou\n",
    "from utils import draw_image_with_box, visualize_anchors_and_gt, decode_predictions, decode_deltas, smooth_l1_loss, bbox_transform\n",
    "from models import RPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cd19760",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES= 2\n",
    "ROI_SIZE= 7\n",
    "SPATIAL_SCALE= 1/8.0\n",
    "ROI_PER_IMG= 128\n",
    "POS_FRACTION= 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06ecd5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondStage(nn.Module):\n",
    "    def __init__(self, in_channels= 512, roi_size= ROI_SIZE, num_classes= NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        self.roi_align= RoIAlign(\n",
    "            output_size= roi_size,\n",
    "            spatial_scale= SPATIAL_SCALE,\n",
    "            sampling_ratio= -1,\n",
    "            aligned= True\n",
    "        )\n",
    "\n",
    "        self.fc= nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_channels * roi_size * roi_size, 1024),\n",
    "            nn.ReLU(inplace= True),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(inplace= True)\n",
    "        )\n",
    "        self.cls_head= nn.Linear(1024, num_classes)\n",
    "        self.reg_head= nn.Linear(1024, 4 * num_classes)\n",
    "    \n",
    "    def forward(self, feature_maps, all_proposals, targets= None):\n",
    "        roi_batch = []\n",
    "        for b, prop in enumerate(all_proposals):\n",
    "            idx = torch.full((prop.shape[0], 1), b, device=prop.device, dtype=prop.dtype)\n",
    "            roi_batch.append(torch.cat([idx, prop], dim=1))\n",
    "        roi_batch = torch.cat(roi_batch, dim=0)          # (N,5)\n",
    "\n",
    "        pooled = self.roi_align(feature_maps, roi_batch)   # (N,C,7,7)\n",
    "        x = self.fc(pooled)                              # (N,1024)\n",
    "        cls_logits = self.cls_head(x)                    # (N,2)\n",
    "        reg_deltas = self.reg_head(x)                    # (N,8)\n",
    "\n",
    "        return self.losses(cls_logits, reg_deltas, roi_batch, targets)\n",
    "    \n",
    "    def losses(self, cls_logits, reg_deltas, roi_batch, targets):\n",
    "        labels_all, reg_targets_all = [], []\n",
    "\n",
    "        for batch, target in enumerate(targets):\n",
    "            mask = roi_batch[:, 0] == batch\n",
    "            proposals = roi_batch[mask, 1:]\n",
    "            iou = calculate_iou(proposals, target['boxes'])\n",
    "            best_iou, best_idx = iou.max(dim=1)\n",
    "\n",
    "            pos = best_iou >= 0.5\n",
    "            labels_b = torch.zeros(proposals.shape[0], dtype=torch.long, device=proposals.device)\n",
    "            labels_b[pos] = 1\n",
    "            labels_all.append(labels_b)\n",
    "\n",
    "            # regression targets for positives only\n",
    "            reg_t = bbox_transform(proposals[pos], target['boxes'][best_idx[pos]])\n",
    "            reg_targets_all.append(reg_t)\n",
    "\n",
    "        labels_all = torch.cat(labels_all)          # (N,)\n",
    "        reg_targets_all = torch.cat(reg_targets_all)  # (N_pos, 4)\n",
    "\n",
    "        # subsample 128 RoIs\n",
    "        pos_idx = torch.where(labels_all == 1)[0]\n",
    "        neg_idx = torch.where(labels_all == 0)[0]\n",
    "        num_pos = max(8, min(len(pos_idx), int(ROI_PER_IMG * POS_FRACTION)))\n",
    "        num_neg = min(len(neg_idx), 128 - num_pos)\n",
    "        keep = torch.cat([pos_idx[:num_pos], neg_idx[:num_neg]])\n",
    "\n",
    "        cls_logits, labels = cls_logits[keep], labels_all[keep]\n",
    "        reg_deltas = reg_deltas[keep]\n",
    "\n",
    "        # slice regression targets for the kept positives\n",
    "        pos_mask = labels == 1\n",
    "        reg_targets = reg_targets_all[:num_pos]  # because we built only positives\n",
    "        cls_loss = nn.functional.cross_entropy(cls_logits, labels)\n",
    "        if pos_mask.sum():\n",
    "            reg_loss = smooth_l1_loss(reg_deltas[pos_mask, 4:8], reg_targets).mean()\n",
    "        else:\n",
    "            torch.tensor(0., device= labels.device)\n",
    "        return {'cls_loss': cls_loss, 'reg_loss': reg_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efbeacd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_backbone= torch.load(artifacts_dir + \"resnet50_backbone.pth\")\n",
    "state_dict= torch.load(artifacts_dir + \"rpn_10epchs_1e-4lr_42anchgt_wghts_bigger_scales.pth\")\n",
    "rpn_model= RPN()\n",
    "rpn_model.load_state_dict(state_dict)\n",
    "for p in resnet50_backbone.parameters():\n",
    "    p.requires_grad= False\n",
    "for p in rpn_model.parameters():\n",
    "    p.requires_grad= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b21f463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['998faa48943fce6f.jpg',\n",
       " 'a228f997057aa291.jpg',\n",
       " '49fe432784afea63.jpg',\n",
       " '0106e273d2de08be.jpg']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_img_extensions= ('.jpg', '.jpeg', '.png')\n",
    "all_images= [\n",
    "    img for img in os.listdir(images_train_dir)\n",
    "    if img.lower().endswith(valid_img_extensions) and os.path.exists(os.path.join(labels_train_dir, img.rsplit('.', 1)[0] + \".txt\"))\n",
    "]\n",
    "all_images[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b60dc13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, val_images= train_test_split(all_images[:1000], test_size= 0.2, random_state= 42)\n",
    "transforms= transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "train_dataset= FaceDataset(image_dir= images_train_dir, label_dir= labels_train_dir, image_list= train_images, transform= transforms)\n",
    "val_dataset= FaceDataset(image_dir= images_train_dir, label_dir= labels_train_dir, image_list= val_images, transform= transforms)\n",
    "train_loader= DataLoader(train_dataset, batch_size= config.BATCH_SIZE, shuffle= True)\n",
    "val_loader= DataLoader(val_dataset, batch_size= config.BATCH_SIZE, shuffle= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bdd0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Objectness Loss: 0.6639 | Train Reg Loss: 0.0141\n",
      "Val Objectness Loss: 0.6503 | Val Regression Loss 0.0133\n",
      "Epoch 2/10: Train Objectness Loss: 0.5768 | Train Reg Loss: 0.0140\n",
      "Val Objectness Loss: 0.6011 | Val Regression Loss 0.0125\n",
      "Epoch 3/10: Train Objectness Loss: 0.5371 | Train Reg Loss: 0.0105\n",
      "Val Objectness Loss: 0.5635 | Val Regression Loss 0.0111\n",
      "Epoch 4/10: Train Objectness Loss: 0.5173 | Train Reg Loss: 0.0086\n",
      "Val Objectness Loss: 0.5471 | Val Regression Loss 0.0102\n",
      "Epoch 5/10: Train Objectness Loss: 0.5664 | Train Reg Loss: 0.0105\n",
      "Val Objectness Loss: 0.5536 | Val Regression Loss 0.0095\n",
      "Epoch 6/10: Train Objectness Loss: 0.5135 | Train Reg Loss: 0.0085\n",
      "Val Objectness Loss: 0.5743 | Val Regression Loss 0.0091\n",
      "Epoch 7/10: Train Objectness Loss: 0.5587 | Train Reg Loss: 0.0065\n",
      "Val Objectness Loss: 0.5190 | Val Regression Loss 0.0086\n",
      "Epoch 8/10: Train Objectness Loss: 0.4461 | Train Reg Loss: 0.0076\n",
      "Val Objectness Loss: 0.5165 | Val Regression Loss 0.0084\n",
      "Epoch 9/10: Train Objectness Loss: 0.4548 | Train Reg Loss: 0.0064\n",
      "Val Objectness Loss: 0.5045 | Val Regression Loss 0.0080\n",
      "Epoch 10/10: Train Objectness Loss: 0.4991 | Train Reg Loss: 0.0067\n",
      "Val Objectness Loss: 0.5007 | Val Regression Loss 0.0082\n"
     ]
    }
   ],
   "source": [
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "second_stage= SecondStage().to(device)\n",
    "\n",
    "rpn_model.to(device)\n",
    "resnet50_backbone.to(device)\n",
    "all_anchors = generate_anchor_boxes(\n",
    "    config.FEATURE_MAP_SHAPE,\n",
    "    config.ANCHOR_SCALES_2,\n",
    "    config.ANCHOR_RATIOS_2,\n",
    "    config.ANCHOR_STRIDE,\n",
    "    config.NUM_ANCHORS_PER_LOC\n",
    ").to(device)\n",
    "optimizer= Adam(second_stage.parameters(), lr= config.LEARNING_RATE)\n",
    "\n",
    "for epoch in range(config.NUM_EPOCHS):\n",
    "    second_stage.train()\n",
    "    train_loss_history, val_loss_history= [], []\n",
    "    for batch in train_loader:\n",
    "        images = batch['image'].to(device)\n",
    "        gt_boxes = [box.to(device) for box in batch['boxes']]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feat = resnet50_backbone(images)          # (B,C,H,W)\n",
    "            cls_logits, reg_deltas = rpn_model(feat) # (B,N,1) & (B,N,4)\n",
    "\n",
    "            # decode to list of proposals\n",
    "            proposals_list = []\n",
    "            for b in range(images.shape[0]):\n",
    "                scores_b = cls_logits[b].squeeze(-1)\n",
    "                deltas_b = reg_deltas[b]\n",
    "                _, prop_b = decode_predictions(\n",
    "                    all_anchors, scores_b, deltas_b,\n",
    "                    pre_nms_topk=1000, post_nms_topk=300,\n",
    "                    cls_score_threshold=0.8, nms_threshold=0.7\n",
    "                )\n",
    "                proposals_list.append(prop_b)\n",
    "\n",
    "        losses = second_stage(feat, proposals_list,\n",
    "                              [{'boxes': b} for b in gt_boxes])\n",
    "        loss = losses['cls_loss'] + losses['reg_loss']\n",
    "        train_loss_history.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{config.NUM_EPOCHS}: Train Objectness Loss: {losses['cls_loss'].item():.4f} | Train Reg Loss: {losses['reg_loss'].item():.4f}\")\n",
    "    second_stage.eval()\n",
    "    val_cls_loss, val_reg_loss, val_samples= 0.0, 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images= batch['image'].to(device)\n",
    "            gt_boxes= [box.to(device) for box in batch['boxes']]\n",
    "\n",
    "            features= resnet50_backbone(images)\n",
    "            cls_logits, reg_deltas= rpn_model(features)\n",
    "\n",
    "            proposals_list= []\n",
    "            for b in range(images.shape[0]):\n",
    "                scores_b= cls_logits[b].squeeze(-1)\n",
    "                deltas_b= reg_deltas[b]\n",
    "                _, proposals_b= decode_predictions(\n",
    "                    all_anchors, scores_b, deltas_b,\n",
    "                    pre_nms_topk= 1000, post_nms_topk=300,\n",
    "                    cls_score_threshold= 0.8, nms_threshold= 0.7\n",
    "                )\n",
    "                proposals_list.append(proposals_b)\n",
    "            losses= second_stage(features, proposals_list,\n",
    "                                 [{'boxes': box} for box in gt_boxes])\n",
    "            \n",
    "            val_cls_loss+= losses['cls_loss'].item()\n",
    "            val_reg_loss+= losses['reg_loss'].item()\n",
    "            val_samples+= 1\n",
    "        val_cls_loss/= val_samples\n",
    "        val_reg_loss/= val_samples\n",
    "        print(f\"Val Objectness Loss: {val_cls_loss:.4f} | Val Regression Loss {val_reg_loss:.4f}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
